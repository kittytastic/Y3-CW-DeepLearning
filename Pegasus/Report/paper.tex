\documentclass{article}

% latex packages, feel free to add more here like 'tikz'
\usepackage{style/conference}
\usepackage{opensans}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{fontawesome}
\usepackage[hidelinks]{hyperref}

% to reference, paste the BibTeX obtained from google scholar into references.bib
\addbibresource{references.bib}
\input{style/math_commands.tex}

% replace this title with your own
\title{Generating Pegasus with a Fully Connected and Variational Autoencoder architecture}

\begin{document}
\maketitle
\begin{abstract}
    This abstract should be short and concise, about 8-10 lines long.
    This paper proposes using a FC-VAE to generate images that look like a Pegasus.  Variational Autoencoders are a powerful class of generative models. Unlike Autoencoders they aim to regualise the distribution of their encoding to ensure that good properties are captured in thier latent space and hence more effectivley can be used to generatee new data. The paper investivates using k-means clustering over latent space for feature extraction. Then usees linear interrpolation between features to geneeatre peegasus images.
\end{abstract}

% this is where the sections begin
\section{Architecture}
\begin{center}
    \includegraphics[width=0.5\textwidth]{figures/Encoder.png}
\end{center}
The network has the general VAE shape. E follows a ResNet \cite{ResNet} architecture with 18 convolution layers. E' is 3 fully connected layers linearly reducing in size in logspace from the output size of the ResNet to latent space size. The latent space is 128 dimensions. D' is again 3 fully connected upscaling to D. D is also based of the ResNet-18 architecture however where ResNet would scale down at the end of a convolutional layer D scales up.



\begin{tabular}{c|c|c|c}
    a&b&c&d\\
    \hline
    a & $\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \times 1$&c&d\\
\end{tabular}


\section{Methodology}
I use the MMD-VAE loss function \cite{infovae}. 
There are 2 terms in the MMD-VAE loss function. Firstly, the reconstruction loss:
\begin{equation}
    \mathcal{L}_{\textrm{recon}} = \mathbb{E}_{p_{\textrm{data}}(x)} \mathbb{E}_{q_\phi (z|x)} [\log p_\theta(x|z)]
\end{equation}

Where $p_\theta(x|z)$ is approximated by networks D', D and parameterized by $\theta$ and likewise $q_\phi(z|x)$ is approximated by networks E, E' parameterized by $\phi$.


Secondly the Maximum Mean Discrepancy \cite{mmd}. Which is used to fit latent codes $z$ to prior $p(z)$.
\begin{equation}
    \textrm{MMD}(p(z) \lVert q(z)) = \mathbb{E}_{p(z), p_(z')}[k(z, z')] + \mathbb{E}_{q(z), q_(z')}[k(z, z')] - 2 \mathbb{E}_{p(z), q_(z')}[k(z, z')]
\end{equation}
where $k(z, z')$ is the gaussian kernel $k(z, z')={\rm e}^{-\frac{\lVert z-z' \rVert^2}{2\sigma^2}}$

The full loss function is:
\begin{equation}
    \mathcal{L}_\textrm{MMD-VAE} = \lambda \textrm{MMD}(q_\phi (z) \lVert p(z)) + \mathcal{L}_{\textrm{recon}}
\end{equation}

where $q_\phi(z) =  \mathbb{E}[q_\phi(z|x)]$ and $\lambda$ is a scaling constant.

Batches of horse and bird images are then subclassified using semi-supervised learning to create an improved dataset of horse and bird images. Random images from these dataset are then lineally interpolated between in latent space and the result decoded to create a pegasus image. 



The method is to train an autoencoder~\cite{kramer1991nonlinear}, by minimising the squared L2 loss:
\begin{equation}
    \mathcal{L}_{\textrm{AE}} = \mathbb{E}_{\vx\sim p_{\textrm{data}}}[\, \lVert \vx - D(E(\vx)) \rVert^2 ]
\end{equation}
The methodology should be very concise and the mathematical notation should try to follow \href{https://v1.overleaf.com/latex/templates/template-for-iclr-2021-conference-submission/mmpfhsxmqdkp.pdf}{the ICLR conference guidlines \faExternalLink}. If you are not familiar with \LaTeX, you can use \href{https://www.codecogs.com/latex/eqneditor.php}{this online \LaTeX~equation editor \faExternalLink}. You may want to include an architectural diagram:
\begin{center}
    \includegraphics[width=0.5\textwidth]{figures/architecture.pdf}
\end{center}
The architectural diagram above was created using Inkscape and exported to a PDF. This was then uploaded to the figures directory on the left.

\section{Class subclassification}
There is a very board range of horse and bird images in CIFAR-10. Some images contain useful features such as white horses or birds' wings. Others images predominantly contain unhelpful features such as beaks. It seems sensible to subclassify the horse and bird classes to give us better images to create a pegasus with. To do this I used semi-supervised learning within the horse and bird classes. Firstly, I clustered a sample of horse/bird pictures in latent space using k-means. Then, using a handful of "good" bird and horse images (\ref{fig:good_horses}), I selected the "good" clusters which should contain many similar features to the example images. If the "good" images fell in different cluster I took the union of these cluster.

I also experimented with clustering over the union of 2 classes and creating subclasses from the union/difference of these classes. The idea being classes may share features. For example, the union of birds and planes could be wings. The difference between between cats and birds could be the set of birds excluding bird faces. Although an interesting idea, due to the limitation of my feature encodings (as talked about in limitations) class intersection was mainly based off global features such as background colour. With this in mind, I found I got better subclassification using the semi-supervised learning approach.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{figures/good_horses.png}
    \end{center}
    \caption{White horses used to select clusters}
    \label{fig:good_horses}
\end{figure}

\section{Generating a Pegasus}
The generation of a pegasus is relteivly simple. 

\section{Results}
The best batch of pegasus images looks like this:
\begin{center}
    \includegraphics[width=0.5\textwidth]{figures/kinda_allright64.png}
\end{center}
From this batch, the most Pegasus-like image is:
\begin{center}
    \includegraphics[width=0.1\textwidth]{figures/kinda_allright1.png}
\end{center}

As you can see the is a wide range of pegasuses. Some look rather good like and others are complete non-sense. Using interpolation made the output pegasus very sensative to its' 2 input images. Although, subclassification improved this it certianly didn't solve the issue.  

\section{Limitations}
The encoder failed to recreate fine details and textures. I susspect this is a comfounding error from: the architectual design; data preperation; training stratery and loss weightings; and insufficent training time for the size of the network.

The subclassification of images was noisy, so although it reduced the domain of horse and bird images it didn't entrly remove unwanted images like ostrich mug shots (of which CIFAR-10 contains many). 

Finally, the method of interrpolation between a horse and a bird to create a pegasus is funemeenattly flawed. A Pegasus is a horse with wings, not a bird horse amalgum. This is talked about more in the improvemnts section.  



\section{Possible Improvements}
There may be some scope for improvement by \textbf{tweaking the ResNet} construction. In the interest of time I used the exact construction as described in \cite{ResNet}. However, Making the ResNet shallower would have helped to speed up training hence more training could have been done in a limited time frame. Also given that the described architecture was developed for the 112x112 ImageNet images its likely not as well suited for 32x32 CIFAR-10 images. In particular the initial 7x7 convolution is likely overkill for CIFAR-10.

\textbf{Cylindrical anealing} of MMD loss. The model uses the constant $\lambda=1$ for the MMD loss. This leads to vanishing MMD loss, much like KL-VAEs suffer vanishing KL-loss. Fu. et.al. \cite{cylindricalAnnealing} proposed the simple technique of cylindrically annealing $\lambda$ which they showed substantially improved reconstruction error. I susspect that cylindrical annealing would benifit MMD-VAEs likewise.

\textbf{Data preparation} could have been improved. I used random horizontal flips of the data but didn't have time to explore any other data preparation techniques. For example introducing color jitter may have helped the VAE to learn more color specific latent features which may have helped creating subclasses. Also random crops and scales may have helped the VAE better learn features such as wings.   

\textbf{Subclasses} definetly added to the final pegasus, filtering out many poor images, however there is still alot of room form Improvements. The subclasses generally differed by color composition rather than feature. For example a black horse a robin and an ostrich face may be clustered as they share the same background. Subclasses could be created using permuted data (random scale,crops e.t.c.). And then ideal images permuted and matched to clusters in a probabilstic manner. This could give much tighter subclasses.

\textbf{Replacing interpolation} would help with creating better pegasus. If you extracted the latent reperesntation of a wing and then applied this to the latent reperesntation of a horse you would likely generate better pegasus. There are many methods you could apply this wing from simple addition, to interpolation to sampling based methods. There are also many methods you could use to extract the latent representation of a wing, I would first investigate using Hierarchical Clustering.


\section*{Bonuses}
This submission has a total bonus of -2 marks as it is only trained on CIFAR-10.

% you can have an unlimited number of references (they can go on the 5th page and span many additional pages without any penalty)
\printbibliography
\end{document}