{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Initialise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZefME0MTvA"
      },
      "source": [
        "# this is a Deep Q Learning (DQN) agent including replay memory and a target network \n",
        "# you can write a brief 8-10 line abstract detailing your submission and experiments here\n",
        "# the code is based on https://github.com/seungeunrho/minimalRL/blob/master/dqn.py, which is released under the MIT licesne\n",
        "# make sure you reference any code you have studied as above, with one comment line per reference\n",
        "\n",
        "# imports\n",
        "import gym\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "video_every   = 25\n",
        "print_every   = 5\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "    \n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "        \n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 256)\n",
        "        self.fc2 = nn.Linear(256, 84)\n",
        "        self.fc3 = nn.Linear(84, env.action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def sample_action(self, obs, epsilon):\n",
        "        out = self.forward(obs)\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,1)\n",
        "        else : \n",
        "            return out.argmax().item()\n",
        "            \n",
        "def train(q, q_target, memory, optimizer):\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
        "\n",
        "        q_out = q(s)\n",
        "        q_a = q_out.gather(1,a)\n",
        "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
        "        target = r + gamma * max_q_prime * done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ck-chjFdScJ"
      },
      "source": [
        "**Train**\n",
        "\n",
        "← You can download the videos from the videos folder in the files on the left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5stHkFq4UztI"
      },
      "source": [
        "# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\n",
        "env = gym.make('Gravitar-ram-v0')\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "\n",
        "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\n",
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "q = QNetwork()\n",
        "q_target = QNetwork()\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "memory = ReplayBuffer()\n",
        "\n",
        "score    = 0.0\n",
        "marking  = []\n",
        "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "\n",
        "for n_episode in range(int(1e32)):\n",
        "    epsilon = max(0.01, 0.08 - 0.01*(n_episode/200)) # linear annealing from 8% to 1%\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    score = 0.0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        a = q.sample_action(torch.from_numpy(s).float().unsqueeze(0), epsilon)\n",
        "        s_prime, r, done, info = env.step(a)\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
        "        s = s_prime\n",
        "\n",
        "        score += r\n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    if memory.size()>2000:\n",
        "        train(q, q_target, memory, optimizer)\n",
        "\n",
        "    # do not change lines 44-48 here, they are for marking the submission log\n",
        "    marking.append(score)\n",
        "    if n_episode%100 == 0:\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "        marking = []\n",
        "\n",
        "    # you can change this part, and print any data you like (so long as it doesn't start with \"marking\")\n",
        "    if n_episode%print_every==0 and n_episode!=0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}